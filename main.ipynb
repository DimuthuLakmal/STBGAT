{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "\n",
    "from data_loader.data_loader import DataLoader\n",
    "from models.sgat_transformer.sgat_transformer import SGATTransformer\n",
    "from test import test\n",
    "from train import train\n",
    "from utils.data_utils import create_lookup_index\n",
    "from utils.logger import logger\n",
    "from utils.masked_mae_loss import Masked_MAE_Loss\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_validate(model, configs: dict, data_loader: DataLoader):\n",
    "    if configs['load_saved_model']:\n",
    "        model.load_state_dict(torch.load(configs['model_input_path']))\n",
    "\n",
    "    # mse_loss_fn = nn.L1Loss()\n",
    "    mse_loss_fn = Masked_MAE_Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer=optimizer, T_0=15, T_mult=1,\n",
    "                                                                        eta_min=0.00005)\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=2, gamma=0.75)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    min_val_loss = np.inf\n",
    "    dec_offset = configs['transformer']['decoder']['seq_offset']\n",
    "\n",
    "    for epoch in range(configs['epochs']):\n",
    "        logger.info(f\"LR: {lr_scheduler.get_last_lr()}\")\n",
    "\n",
    "        mae_train_loss, rmse_train_loss, mape_train_loss = train(model=model,\n",
    "                                                                 data_loader=data_loader,\n",
    "                                                                 optimizer=optimizer,\n",
    "                                                                 loss_fn=mse_loss_fn,\n",
    "                                                                 device=configs['device'],\n",
    "                                                                 seq_offset=dec_offset)\n",
    "\n",
    "        mae_val_loss, rmse_val_loss, mape_val_loss = test(_type='test',\n",
    "                                                          model=model,\n",
    "                                                          data_loader=data_loader,\n",
    "                                                          device=configs['device'],\n",
    "                                                          seq_offset=dec_offset)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        out_txt = f\"Epoch: {epoch} | mae_train_loss: {mae_train_loss} | rmse_train_loss: {rmse_train_loss} \" \\\n",
    "                  f\"| mape_train_loss: {mape_train_loss} | mae_val_loss: {mae_val_loss} \" \\\n",
    "                  f\"| rmse_val_loss: {rmse_val_loss} | mape_val_loss: {mape_val_loss}\"\n",
    "        logger.info(out_txt)\n",
    "\n",
    "        if min_val_loss > mae_val_loss:\n",
    "            min_val_loss = mae_val_loss\n",
    "            logger.info('Saving Model...')\n",
    "            best_model_path = configs['model_output_path'].format(str(epoch))\n",
    "            torch.save(model.state_dict(), best_model_path)  # saving model\n",
    "\n",
    "    # testing model\n",
    "    logger.info('Testing model...')\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    mae_test_loss, rmse_test_loss, mape_test_loss = test(_type='test',\n",
    "                                                         model=model,\n",
    "                                                         data_loader=data_loader,\n",
    "                                                         device=configs['device'],\n",
    "                                                         seq_offset=dec_offset)\n",
    "\n",
    "    logger.info(f\"mae_test_loss: {mae_test_loss} | rmse_test_loss: {rmse_test_loss} | mape_test_loss: {mape_test_loss}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prepare_data(model_configs: dict, data_configs: dict):\n",
    "    data_configs['batch_size'] = model_configs['batch_size']\n",
    "    data_configs['enc_features'] = model_configs['transformer']['encoder']['features']\n",
    "    data_configs['dec_seq_offset'] = model_configs['transformer']['decoder']['seq_offset']\n",
    "    dec_seq_len = model_configs['transformer']['decoder']['seq_len']\n",
    "    enc_seq_len = model_configs['transformer']['encoder']['seq_len']\n",
    "\n",
    "    data_loader = DataLoader(data_configs)\n",
    "    data_loader.load_node_data_file()\n",
    "    edge_index, edge_attr = data_loader.load_edge_data_file()\n",
    "    edge_details = data_loader.load_semantic_edge_data_file()\n",
    "\n",
    "    model_configs['transformer']['decoder']['edge_index'] = edge_index\n",
    "    model_configs['transformer']['decoder']['edge_attr'] = edge_attr\n",
    "    model_configs['transformer']['decoder']['edge_details'] = edge_details\n",
    "\n",
    "    model_configs['transformer']['encoder']['edge_index'] = edge_index\n",
    "    model_configs['transformer']['encoder']['edge_attr'] = edge_attr\n",
    "    model_configs['transformer']['encoder']['edge_details'] = edge_details\n",
    "    model_configs['transformer']['encoder']['num_of_weeks'] = data_configs['num_of_weeks']\n",
    "    model_configs['transformer']['encoder']['num_of_days'] = data_configs['num_of_days']\n",
    "    model_configs['transformer']['encoder']['basic_input_len'] = data_configs['len_input']\n",
    "    model_configs['transformer']['encoder']['points_per_hour'] = data_configs['points_per_hour']\n",
    "    model_configs['transformer']['encoder']['num_days_per_week'] = data_configs['num_days_per_week']\n",
    "\n",
    "    max_lkup_len_enc, lkup_idx_enc, max_lkup_len_dec, lkup_idx_dec = create_lookup_index(data_configs['num_of_weeks'],\n",
    "                                                                                         data_configs['num_of_days'],\n",
    "                                                                                         data_configs['dec_seq_offset'],\n",
    "                                                                                         dec_seq_len)\n",
    "\n",
    "    model_configs['transformer']['decoder']['lookup_idx'] = lkup_idx_dec\n",
    "    model_configs['transformer']['decoder']['max_lookup_len'] = max_lkup_len_dec if max_lkup_len_dec else dec_seq_len\n",
    "    model_configs['transformer']['encoder']['lookup_idx'] = lkup_idx_enc\n",
    "    model_configs['transformer']['encoder']['max_lookup_len'] = max_lkup_len_enc if max_lkup_len_enc else enc_seq_len\n",
    "\n",
    "    return data_loader, model_configs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # load configs\n",
    "    with open(\"config/config.yaml\", \"r\") as stream:\n",
    "        configs = yaml.safe_load(stream)\n",
    "\n",
    "    model_configs = configs['model']\n",
    "    data_configs = configs['data']\n",
    "    data_loader, model_configs = prepare_data(model_configs, data_configs)\n",
    "\n",
    "    model = SGATTransformer(configs=model_configs).to(model_configs['device'])\n",
    "    train_validate(model, model_configs, data_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
