{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "\n",
    "from data_loader.data_loader import DataLoader\n",
    "from models.sgat_transformer.sgat_transformer import SGATTransformer\n",
    "from test import test\n",
    "from train import train\n",
    "from utils.data_utils import create_lookup_index\n",
    "from utils.logger import logger\n",
    "from utils.masked_mae_loss import Masked_MAE_Loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_validate(model: SGATTransformer,\n",
    "                   configs: dict,\n",
    "                   lr: float,\n",
    "                   ls_fn: torch.nn.Module,\n",
    "                   is_lr_sh: bool = True,\n",
    "                   _train: bool = True):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if is_lr_sh:\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer=optimizer, T_0=20, T_mult=1,\n",
    "                                                                            eta_min=0.00001)\n",
    "\n",
    "    best_model_path = None\n",
    "    min_val_loss = np.inf\n",
    "    dec_offset = configs['transformer']['decoder']['seq_offset']\n",
    "    epochs = configs['train_epochs'] if _train else configs['finetune_epochs']\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if is_lr_sh:\n",
    "            logger.info(f\"LR: {lr_scheduler.get_last_lr()}\")\n",
    "\n",
    "        mae_train_loss, rmse_train_loss, mape_train_loss = train(model=model,\n",
    "                                                                 data_loader=data_loader,\n",
    "                                                                 optimizer=optimizer,\n",
    "                                                                 loss_fn=ls_fn,\n",
    "                                                                 device=configs['device'],\n",
    "                                                                 seq_offset=dec_offset,\n",
    "                                                                 _train=_train)\n",
    "\n",
    "        mae_val_loss, rmse_val_loss, mape_val_loss = test(_type='val',\n",
    "                                                          model=model,\n",
    "                                                          data_loader=data_loader,\n",
    "                                                          device=configs['device'],\n",
    "                                                          seq_offset=dec_offset)\n",
    "        if is_lr_sh:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        out_txt = f\"Epoch: {epoch} | mae_train_loss: {mae_train_loss} | rmse_train_loss: {rmse_train_loss} \" \\\n",
    "                  f\"| mape_train_loss: {mape_train_loss} | mae_val_loss: {mae_val_loss} \" \\\n",
    "                  f\"| rmse_val_loss: {rmse_val_loss} | mape_val_loss: {mape_val_loss}\"\n",
    "        logger.info(out_txt)\n",
    "\n",
    "        if min_val_loss > mae_val_loss:\n",
    "            min_val_loss = mae_val_loss\n",
    "            logger.info('Saving Model...')\n",
    "            best_model_path = configs['model_output_path'].format(str(epoch))\n",
    "            torch.save(model.state_dict(), best_model_path)  # saving model\n",
    "\n",
    "    return best_model_path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run(model: SGATTransformer, configs: dict, data_loader: DataLoader):\n",
    "    \"\"\"\n",
    "    Train the model and save the model with the best performance for val dataset. Test the best model with test dataset.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: SGATTransformer\n",
    "    configs: dict, model_configs\n",
    "    data_loader: DataLoader\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    if configs['load_saved_model']:\n",
    "        model.load_state_dict(torch.load(configs['model_input_path']))\n",
    "\n",
    "    mse_loss_fn = Masked_MAE_Loss()\n",
    "\n",
    "    # Initial Training\n",
    "    logger.info('Training model...')\n",
    "    train_validate(model=model,\n",
    "                   configs=configs,\n",
    "                   lr=0.001,\n",
    "                   ls_fn=mse_loss_fn,\n",
    "                   is_lr_sh=True,\n",
    "                   _train=True)\n",
    "\n",
    "    # Fine tuning\n",
    "    best_model_path = train_validate(model=model,\n",
    "                                     configs=configs,\n",
    "                                     lr=0.0005,\n",
    "                                     ls_fn=mse_loss_fn,\n",
    "                                     is_lr_sh=True,\n",
    "                                     _train=False)\n",
    "\n",
    "    # testing model\n",
    "    logger.info('Testing model...')\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    dec_offset = configs['transformer']['decoder']['seq_offset']\n",
    "    mae_test_loss, rmse_test_loss, mape_test_loss = test(_type='test',\n",
    "                                                         model=model,\n",
    "                                                         data_loader=data_loader,\n",
    "                                                         device=configs['device'],\n",
    "                                                         seq_offset=dec_offset)\n",
    "\n",
    "    logger.info(f\"mae_test_loss: {mae_test_loss} | rmse_test_loss: {rmse_test_loss} | mape_test_loss: {mape_test_loss}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prepare_data(model_configs: dict, data_configs: dict):\n",
    "    \"\"\"\n",
    "    Carry out data preprocessing and modifications to model_configs and data_configs.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_configs: dict, model hyperparameters\n",
    "    data_configs: dict, data configurations\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_loader: DataLoader, configured data_loader\n",
    "    model_configs: dict, modified model_configs dict\n",
    "    \"\"\"\n",
    "    data_configs['batch_size'] = model_configs['batch_size']\n",
    "    data_configs['enc_features'] = model_configs['transformer']['encoder']['features']\n",
    "    data_configs['dec_seq_offset'] = model_configs['transformer']['decoder']['seq_offset']\n",
    "    dec_seq_len = model_configs['transformer']['decoder']['seq_len']\n",
    "    enc_seq_len = model_configs['transformer']['encoder']['seq_len']\n",
    "\n",
    "    data_configs['time_idx_enc_feature'] = True if model_configs['transformer']['encoder']['input_dim'] == 2 else False\n",
    "    data_configs['time_idx_dec_feature'] = True if model_configs['transformer']['decoder']['input_dim'] == 2 else False\n",
    "    data_loader = DataLoader(data_configs)\n",
    "    data_loader.load_node_data_file()\n",
    "    edge_index, edge_attr = data_loader.load_edge_data_file()\n",
    "    sem_edge_details = [[], []]\n",
    "    if model_configs['transformer']['encoder']['graph_semantic_input']:\n",
    "        sem_edge_details = data_loader.load_semantic_edge_data_file()\n",
    "\n",
    "    model_configs['transformer']['encoder']['edge_index'] = edge_index\n",
    "    model_configs['transformer']['encoder']['edge_attr'] = edge_attr\n",
    "    model_configs['transformer']['encoder']['sem_edge_details'] = sem_edge_details\n",
    "\n",
    "    max_lkup_len_enc, lkup_idx_enc, max_lkup_len_dec, lkup_idx_dec = create_lookup_index(data_configs['num_of_weeks'],\n",
    "                                                                                         data_configs['num_of_days'],\n",
    "                                                                                         data_configs['dec_seq_offset'],\n",
    "                                                                                         dec_seq_len)\n",
    "\n",
    "    model_configs['transformer']['decoder']['lookup_idx'] = lkup_idx_dec\n",
    "    model_configs['transformer']['decoder']['max_lookup_len'] = max_lkup_len_dec if max_lkup_len_dec else dec_seq_len\n",
    "    model_configs['transformer']['encoder']['lookup_idx'] = lkup_idx_enc\n",
    "    model_configs['transformer']['encoder']['max_lookup_len'] = max_lkup_len_enc if max_lkup_len_enc else enc_seq_len\n",
    "\n",
    "    return data_loader, model_configs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    with open(\"config/config.yaml\", \"r\") as stream:\n",
    "        configs = yaml.safe_load(stream)\n",
    "\n",
    "    model_configs = configs['model']\n",
    "    data_configs = configs['data']\n",
    "    data_loader, model_configs = prepare_data(model_configs, data_configs)\n",
    "\n",
    "    model = SGATTransformer(configs=model_configs).to(model_configs['device'])\n",
    "    run(model, model_configs, data_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
